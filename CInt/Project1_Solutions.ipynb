{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e94e8f",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "       In order to prepare data we have to split them into 2 sets: train, validation. Train set is used to train a MLP model, and validation is used for evaluate results of models. Test set will be used from different data to compare best MLP with Fuzzy set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac0f42a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "raw_data = pd.read_csv(\"ACI21-22_Proj1IoTGatewayCrashDataset.csv\")\n",
    "X = raw_data.drop('Falha', axis=1).to_numpy()\n",
    "y = raw_data['Falha'].to_numpy()\n",
    "\n",
    "#Splitting the data for train, validation and test set (80%, 20%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,  shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62a2880",
   "metadata": {},
   "source": [
    "# Problem 3 Finding hyperparameters\n",
    "\n",
    "    Table below presents different hyperparameters used during training of models. Unfortunetly GridSearchCV from sklearn package uses cross validation which can't be used in our example because of sequential data, where order matters. That's why we have to search of hyperparameters by outselfs.\n",
    "\n",
    "    In previous versions different hyperparameters were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92717f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "clfs = {\n",
    "    #'adam_2': MLPClassifier(hidden_layer_sizes=(2,), max_iter=1000, random_state=1234),\n",
    "    #'adam2_8': MLPClassifier(hidden_layer_sizes=(16,), max_iter=1000, random_state=1234),\n",
    "    'adam3_32': MLPClassifier(hidden_layer_sizes=(32,), max_iter=1000, random_state=1234),\n",
    "    #'adam1_2lay': MLPClassifier(hidden_layer_sizes=(32,16), max_iter=1000, random_state=1234),\n",
    "    'adam2_2lay': MLPClassifier(hidden_layer_sizes=(40,20), max_iter=1000, random_state=1234),\n",
    "    #'sgd1_2lay': MLPClassifier(hidden_layer_sizes=(2,),solver='sgd',max_iter=1000,random_state=1234),\n",
    "    #'sgd2_8lay': MLPClassifier(hidden_layer_sizes=(8,),solver='sgd',max_iter=1000, momentum=0.9,random_state=1234),\n",
    "    #'sgd3_32lay': MLPClassifier(hidden_layer_sizes=(32,),solver='sgd',max_iter=1000, momentum=0.9,random_state=1234),\n",
    "    #'sgd1_adaptive_momentum': MLPClassifier(hidden_layer_sizes=(2,),solver='sgd',max_iter=1000, momentum=0,random_state=1234),\n",
    "    #'sgd2_momentum': MLPClassifier(hidden_layer_sizes=(8,),solver='sgd',max_iter=1000, momentum=0.9,random_state=1234),\n",
    "    #'sgd3_momentum': MLPClassifier(hidden_layer_sizes=(32,),solver='sgd',max_iter=1000, momentum=0.9,random_state=1234),\n",
    "    #'lbfgs_no_momentum': MLPClassifier(hidden_layer_sizes=(2,), solver='lbfgs', max_iter=1000,random_state=1234),\n",
    "    #'lbfgs_momentum': MLPClassifier(hidden_layer_sizes=(8,), solver='lbfgs', max_iter=1000, random_state=1234),\n",
    "    'lbfgs_one_layer': MLPClassifier(hidden_layer_sizes=(32,), solver='lbfgs', max_iter=1000, random_state=1234),\n",
    "    'lbfgs_two_layers': MLPClassifier(hidden_layer_sizes=(32,20), solver='lbfgs', max_iter=1000, random_state=1234),\n",
    "}\n",
    "def TrainModel(X_train, X_val, y_train, y_val):\n",
    "    for clf_id, clf_name in enumerate(clfs):\n",
    "        clf = clfs[clf_name]\n",
    "        clf.fit(X_train, y_train)\n",
    "        prediction = clf.predict(X_val)\n",
    "        print(clf_name)\n",
    "        print(\"Confusion matrix(tn, fp, fn, tp):\", confusion_matrix(y_val, prediction).ravel())\n",
    "        #print(\"Accuracy:\",accuracy_score(y_val, prediction)) Accuracy is useless in our case\n",
    "        print(\"Precision:\",precision_score(y_val, prediction ,zero_division=0))\n",
    "        print(\"Recall:\",recall_score(y_val, prediction))\n",
    "        print(\"F1 score:\",f1_score(y_val, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5a48937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adam3_32\n",
      "Confusion matrix(tn, fp, fn, tp): [389   0   4   7]\n",
      "Precision: 1.0\n",
      "Recall: 0.6363636363636364\n",
      "F1 score: 0.7777777777777778\n",
      "adam2_2lay\n",
      "Confusion matrix(tn, fp, fn, tp): [388   1   2   9]\n",
      "Precision: 0.9\n",
      "Recall: 0.8181818181818182\n",
      "F1 score: 0.8571428571428572\n",
      "lbfgs_one_layer\n",
      "Confusion matrix(tn, fp, fn, tp): [385   4   0  11]\n",
      "Precision: 0.7333333333333333\n",
      "Recall: 1.0\n",
      "F1 score: 0.846153846153846\n",
      "lbfgs_two_layers\n",
      "Confusion matrix(tn, fp, fn, tp): [384   5   0  11]\n",
      "Precision: 0.6875\n",
      "Recall: 1.0\n",
      "F1 score: 0.8148148148148148\n"
     ]
    }
   ],
   "source": [
    "TrainModel(X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ea3563",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "Using simple MLP is not enough, best result are obtained by using solver lbfgs, which uses quasi-Newton methods. Looking at accuracy doesn't make sense, because to have high value of accuracy we don't even have to train a model. Neverless we should keep eye on recall and precision. Recall, which tells us about situation when there was a crash and system didn't detect is most crucial error that we care about. Our goal is to detect all crashes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd71e6f6",
   "metadata": {},
   "source": [
    "# Problem 4:\n",
    "\n",
    "       In order to have information about previous request, function createPreviousumberOfRequestData(n) adds to our X_train set addition columns with information about previous request: Number_of_Requests(t-1), Number_of_Requests(t-2),...,Number_of_Requests(t-n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8eb57938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createPreviousumberOfRequestData(data, n):\n",
    "    x = data.iloc[:, 1].to_numpy()\n",
    "    req = data.iloc[:, :2].to_numpy()\n",
    "    x = np.vstack((x,req[:,0]))\n",
    "    for i in range(n):\n",
    "        requestN =np.roll(req[:,0],i+1)\n",
    "        x = np.vstack((x,requestN))\n",
    "        \n",
    "    return x.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9896a273",
   "metadata": {},
   "source": [
    "    By checking past request we tune the hyperameter n, which changes size of input layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "fdfe4c66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of request T- 1\n",
      "adam3_32\n",
      "Confusion matrix(tn, fp, fn, tp): [388   1   6   5]\n",
      "Precision: 0.8333333333333334\n",
      "Recall: 0.45454545454545453\n",
      "F1 score: 0.5882352941176471\n",
      "adam2_2lay\n",
      "Confusion matrix(tn, fp, fn, tp): [385   4   3   8]\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.7272727272727273\n",
      "F1 score: 0.6956521739130435\n",
      "lbfgs_one_layer\n",
      "Confusion matrix(tn, fp, fn, tp): [386   3   0  11]\n",
      "Precision: 0.7857142857142857\n",
      "Recall: 1.0\n",
      "F1 score: 0.88\n",
      "lbfgs_two_layers\n",
      "Confusion matrix(tn, fp, fn, tp): [384   5   1  10]\n",
      "Precision: 0.6666666666666666\n",
      "Recall: 0.9090909090909091\n",
      "F1 score: 0.7692307692307692\n",
      "\n",
      "Number of request T- 2\n",
      "adam3_32\n",
      "Confusion matrix(tn, fp, fn, tp): [387   2   5   6]\n",
      "Precision: 0.75\n",
      "Recall: 0.5454545454545454\n",
      "F1 score: 0.631578947368421\n",
      "adam2_2lay\n",
      "Confusion matrix(tn, fp, fn, tp): [385   4   1  10]\n",
      "Precision: 0.7142857142857143\n",
      "Recall: 0.9090909090909091\n",
      "F1 score: 0.8\n",
      "lbfgs_one_layer\n",
      "Confusion matrix(tn, fp, fn, tp): [383   6   3   8]\n",
      "Precision: 0.5714285714285714\n",
      "Recall: 0.7272727272727273\n",
      "F1 score: 0.64\n",
      "lbfgs_two_layers\n",
      "Confusion matrix(tn, fp, fn, tp): [383   6   2   9]\n",
      "Precision: 0.6\n",
      "Recall: 0.8181818181818182\n",
      "F1 score: 0.6923076923076923\n",
      "\n",
      "Number of request T- 3\n",
      "adam3_32\n",
      "Confusion matrix(tn, fp, fn, tp): [389   0   3   8]\n",
      "Precision: 1.0\n",
      "Recall: 0.7272727272727273\n",
      "F1 score: 0.8421052631578948\n",
      "adam2_2lay\n",
      "Confusion matrix(tn, fp, fn, tp): [389   0   1  10]\n",
      "Precision: 1.0\n",
      "Recall: 0.9090909090909091\n",
      "F1 score: 0.9523809523809523\n",
      "lbfgs_one_layer\n",
      "Confusion matrix(tn, fp, fn, tp): [388   1   0  11]\n",
      "Precision: 0.9166666666666666\n",
      "Recall: 1.0\n",
      "F1 score: 0.9565217391304348\n",
      "lbfgs_two_layers\n",
      "Confusion matrix(tn, fp, fn, tp): [387   2   0  11]\n",
      "Precision: 0.8461538461538461\n",
      "Recall: 1.0\n",
      "F1 score: 0.9166666666666666\n",
      "\n",
      "Number of request T- 4\n",
      "adam3_32\n",
      "Confusion matrix(tn, fp, fn, tp): [389   0   2   9]\n",
      "Precision: 1.0\n",
      "Recall: 0.8181818181818182\n",
      "F1 score: 0.9\n",
      "adam2_2lay\n",
      "Confusion matrix(tn, fp, fn, tp): [389   0   1  10]\n",
      "Precision: 1.0\n",
      "Recall: 0.9090909090909091\n",
      "F1 score: 0.9523809523809523\n",
      "lbfgs_one_layer\n",
      "Confusion matrix(tn, fp, fn, tp): [388   1   2   9]\n",
      "Precision: 0.9\n",
      "Recall: 0.8181818181818182\n",
      "F1 score: 0.8571428571428572\n",
      "lbfgs_two_layers\n",
      "Confusion matrix(tn, fp, fn, tp): [387   2   1  10]\n",
      "Precision: 0.8333333333333334\n",
      "Recall: 0.9090909090909091\n",
      "F1 score: 0.8695652173913043\n",
      "\n",
      "Number of request T- 5\n",
      "adam3_32\n",
      "Confusion matrix(tn, fp, fn, tp): [388   1   2   9]\n",
      "Precision: 0.9\n",
      "Recall: 0.8181818181818182\n",
      "F1 score: 0.8571428571428572\n",
      "adam2_2lay\n",
      "Confusion matrix(tn, fp, fn, tp): [388   1   2   9]\n",
      "Precision: 0.9\n",
      "Recall: 0.8181818181818182\n",
      "F1 score: 0.8571428571428572\n",
      "lbfgs_one_layer\n",
      "Confusion matrix(tn, fp, fn, tp): [387   2   3   8]\n",
      "Precision: 0.8\n",
      "Recall: 0.7272727272727273\n",
      "F1 score: 0.761904761904762\n",
      "lbfgs_two_layers\n",
      "Confusion matrix(tn, fp, fn, tp): [388   1   2   9]\n",
      "Precision: 0.9\n",
      "Recall: 0.8181818181818182\n",
      "F1 score: 0.8571428571428572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    print(\"Number of request T-\",i)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(createPreviousumberOfRequestData(raw_data, i), y, test_size=0.2,  shuffle=False)\n",
    "    TrainModel(X_train, X_val, y_train, y_val)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68a9196",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "Best results:\n",
    "\n",
    "Obtainted with 3 additional request from the previous timesteps:\n",
    "\n",
    "    One hidden layer LBFGS\n",
    "    Confusion matrix(tn, fp, fn, tp): [388   1   0  11]\n",
    "    Precision: 0.9166666666666666\n",
    "    Recall: 1.0\n",
    "    F1 score: 0.9565217391304348\n",
    "    \n",
    "    Two layers LBFGS\n",
    "    Confusion matrix(tn, fp, fn, tp): [387   2   0  11]\n",
    "    Precision: 0.8461538461538461\n",
    "    Recall: 1.0\n",
    "    F1 score: 0.9166666666666666\n",
    "    \n",
    "Best result are obtained by looking at 3 past results by solver lbfgs. Using advice from second expert we were able to detect all crashes ( recall = 1 ). Using additional previous data (t-4) makes our model worse Recall = 0.9090909090909091 and Recall= 0.8181818181818182 for lbfgs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586b2bdf",
   "metadata": {},
   "source": [
    "# Problem 5\n",
    "\n",
    "Using advice from second expert and instead of using the number of requests as the inputs for the system, we will try to create new feature with them. Our X = {load, normalized sum of t- request)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd004051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adam3_32\n",
      "Confusion matrix(tn, fp, fn, tp): [389   0   4   7]\n",
      "Precision: 1.0\n",
      "Recall: 0.6363636363636364\n",
      "F1 score: 0.7777777777777778\n",
      "adam2_2lay\n",
      "Confusion matrix(tn, fp, fn, tp): [388   1   2   9]\n",
      "Precision: 0.9\n",
      "Recall: 0.8181818181818182\n",
      "F1 score: 0.8571428571428572\n",
      "lbfgs_one_layer\n",
      "Confusion matrix(tn, fp, fn, tp): [385   4   0  11]\n",
      "Precision: 0.7333333333333333\n",
      "Recall: 1.0\n",
      "F1 score: 0.846153846153846\n",
      "lbfgs_two_layers\n",
      "Confusion matrix(tn, fp, fn, tp): [384   5   0  11]\n",
      "Precision: 0.6875\n",
      "Recall: 1.0\n",
      "F1 score: 0.8148148148148148\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "def transform(X):\n",
    "    e = np.sum(X[:, 1:5],axis=1)\n",
    "    result = np.vstack((X[:, 0],e)).T\n",
    "    scaler.fit_transform(result)\n",
    "    return result\n",
    "\n",
    "X = transform(createPreviousumberOfRequestData(raw_data, 3))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,  shuffle=False)\n",
    "TrainModel(X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69243e3c",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "    Best results:\n",
    "    \n",
    "    Two layers Adam:\n",
    "    Confusion matrix(tn, fp, fn, tp): [388   1   2   9]\n",
    "    Precision: 0.9\n",
    "    Recall: 0.8181818181818182\n",
    "    F1 score: 0.8571428571428572\n",
    "    \n",
    "    One layer lbfgs:\n",
    "    Confusion matrix(tn, fp, fn, tp): [385   4   0  11]\n",
    "    Precision: 0.7333333333333333\n",
    "    Recall: 1.0\n",
    "    F1 score: 0.846153846153846\n",
    "    \n",
    "    Two layers lbfgs:\n",
    "    Confusion matrix(tn, fp, fn, tp): [384   5   0  11]\n",
    "    Precision: 0.6875\n",
    "    Recall: 1.0\n",
    "    F1 score: 0.8148148148148148\n",
    "\n",
    "Once again we recieve best result from two layers lbfgs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf04910",
   "metadata": {},
   "source": [
    "# Fuzzy Rule Based Expert System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a37edba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skfuzzy as fuzz\n",
    "import matplotlib.pyplot as plt\n",
    "from skfuzzy import control as ctrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e5430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load = ctrl.Antecedent(np.arange(0, 1, 0.01), 'load')\n",
    "sum_request = ctrl.Antecedent(np.arange(0, 1, 0.01), 'sum_request')\n",
    "crash = ctrl.Consequent(np.arange(0, 2, 1), 'crash')\n",
    "\n",
    "load['low'] = fuzz.trimf(load.universe, [0, 0, 0.5])\n",
    "load['medium'] = fuzz.trimf(load.universe, [0, 0.5, 1])\n",
    "load['high'] = fuzz.trimf(load.universe, [0.5, 1, 1])\n",
    "sum_request['low'] = fuzz.trimf(sum_request.universe, [0, 0, 0.5])\n",
    "sum_request['medium'] = fuzz.trimf(sum_request.universe, [0, 0.5, 1])\n",
    "sum_request['high'] = fuzz.trimf(sum_request.universe, [0.5, 1, 1])\n",
    "crash['no'] = fuzz.trimf(crash.universe, [0, 0, 0.4])\n",
    "crash['yes'] = fuzz.trimf(crash.universe, [0.4, 1, 1])\n",
    "\n",
    "rule1 = ctrl.Rule(load['high'] & sum_request['high'], crash['yes'])\n",
    "rule2 = ctrl.Rule(load['low'] & sum_request['low'], crash['no'])\n",
    "rule3 = ctrl.Rule(load['low'] & sum_request['medium'], crash['no'])\n",
    "rule4 = ctrl.Rule(load['low'] & sum_request['high'], crash['no'])\n",
    "rule5 = ctrl.Rule(load['medium'] & sum_request['low'], crash['no'])\n",
    "rule6 = ctrl.Rule(load['medium'] & sum_request['medium'], crash['no'])\n",
    "rule7 = ctrl.Rule(load['medium'] & sum_request['high'], crash['no'])\n",
    "rule8 = ctrl.Rule(load['high'] & sum_request['low'], crash['no'])\n",
    "rule9 = ctrl.Rule(load['high'] & sum_request['medium'], crash['no'])\n",
    "\n",
    "crash_ctrl = ctrl.ControlSystem([rule1, rule2, rule3, rule4, rule5, rule6, rule7, rule8, rule9])\n",
    "\n",
    "crashing = ctrl.ControlSystemSimulation(crash_ctrl)\n",
    "\n",
    "#Testing \n",
    "crashing.input['load'] = 0.8\n",
    "crashing.input['sum_request'] = 0.8\n",
    "crashing.compute()\n",
    "\n",
    "print(crashing.output['crash'])\n",
    "crash.view(sim=crashing)\n",
    "#We got binary problem and want binary output thay's why we have to use crisp output. Defuzzification of \n",
    "print(fuzz.lambda_cut(crashing.output['crash'], 0.5))\n",
    "X = transform(createPreviousumberOfRequestData(raw_data, 3))\n",
    "y = raw_data['Falha'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17d1138",
   "metadata": {},
   "source": [
    "# Generalization\n",
    "\n",
    "In order to validate the models we will use different set of data (*ACI_Proj1_TestSet.csv*) to compare models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfc3e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"ACI_Proj1_TestSet.csv\",header=None)\n",
    "X_test = test.iloc[:, :2].to_numpy()\n",
    "y_test = test.iloc[:, 2].to_numpy()\n",
    "\n",
    "\n",
    "X = raw_data.drop('Falha', axis=1).to_numpy()\n",
    "y = raw_data['Falha'].to_numpy()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,  shuffle=False)\n",
    "\n",
    "# Doing training once again, but on the same training set as previous. Not including new data set. \n",
    "# As a result of setting the same random seed models will be the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "0c2e2c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adam3_32\n",
      "Confusion matrix(tn, fp, fn, tp): [188   4   7   1]\n",
      "Precision: 0.2\n",
      "Recall: 0.125\n",
      "F1 score: 0.15384615384615385\n",
      "adam2_2lay\n",
      "Confusion matrix(tn, fp, fn, tp): [187   5   7   1]\n",
      "Precision: 0.16666666666666666\n",
      "Recall: 0.125\n",
      "F1 score: 0.14285714285714288\n",
      "lbfgs_one_layer\n",
      "Confusion matrix(tn, fp, fn, tp): [190   2   2   6]\n",
      "Precision: 0.75\n",
      "Recall: 0.75\n",
      "F1 score: 0.75\n",
      "lbfgs_two_layers\n",
      "Confusion matrix(tn, fp, fn, tp): [188   4   3   5]\n",
      "Precision: 0.5555555555555556\n",
      "Recall: 0.625\n",
      "F1 score: 0.5882352941176471\n"
     ]
    }
   ],
   "source": [
    "# MLP without advices from experts\n",
    "TrainModel(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d018ec4",
   "metadata": {},
   "source": [
    "###### Results\n",
    "Model without modification doesn't work great. Regardless two layers MLP was able to detect 60% of crashes correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a55f8b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adam3_32\n",
      "Confusion matrix(tn, fp, fn, tp): [192   0   1   7]\n",
      "Precision: 1.0\n",
      "Recall: 0.875\n",
      "F1 score: 0.9333333333333333\n",
      "adam2_2lay\n",
      "Confusion matrix(tn, fp, fn, tp): [191   1   0   8]\n",
      "Precision: 0.8888888888888888\n",
      "Recall: 1.0\n",
      "F1 score: 0.9411764705882353\n",
      "lbfgs_one_layer\n",
      "Confusion matrix(tn, fp, fn, tp): [186   6   1   7]\n",
      "Precision: 0.5384615384615384\n",
      "Recall: 0.875\n",
      "F1 score: 0.6666666666666667\n",
      "lbfgs_two_layers\n",
      "Confusion matrix(tn, fp, fn, tp): [190   2   1   7]\n",
      "Precision: 0.7777777777777778\n",
      "Recall: 0.875\n",
      "F1 score: 0.823529411764706\n"
     ]
    }
   ],
   "source": [
    "# Problem 4\n",
    "X_train, X_val, y_train, y_val = train_test_split(createPreviousumberOfRequestData(raw_data, 3), y, test_size=0.2,  shuffle=False)\n",
    "TrainModel(X_train, createPreviousumberOfRequestData(test, 3), y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a7b4f9",
   "metadata": {},
   "source": [
    "###### Results\n",
    "    Two layers adam:\n",
    "    Confusion matrix(tn, fp, fn, tp): [191   1   0   8]\n",
    "    Precision: 0.8888888888888888\n",
    "    Recall: 1.0\n",
    "    F1 score: 0.9411764705882353\n",
    "    \n",
    "    One layers lbfgs:\n",
    "    Confusion matrix(tn, fp, fn, tp): [186   6   1   7]\n",
    "    Precision: 0.5384615384615384\n",
    "    Recall: 0.875\n",
    "    F1 score: 0.6666666666666667\n",
    "    \n",
    "    Two layers lbfgs:\n",
    "    Confusion matrix(tn, fp, fn, tp): [190   2   1   7]\n",
    "    Precision: 0.7777777777777778\n",
    "    Recall: 0.875\n",
    "    F1 score: 0.823529411764706\n",
    "Adding n new columns of previous requests at timestep t-n as inputs works very well. We got high recall with only 1-2 missed crashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "c4839a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adam3_32\n",
      "Confusion matrix(tn, fp, fn, tp): [191   1   3   5]\n",
      "Precision: 0.8333333333333334\n",
      "Recall: 0.625\n",
      "F1 score: 0.7142857142857143\n",
      "adam2_2lay\n",
      "Confusion matrix(tn, fp, fn, tp): [190   2   1   7]\n",
      "Precision: 0.7777777777777778\n",
      "Recall: 0.875\n",
      "F1 score: 0.823529411764706\n",
      "lbfgs_one_layer\n",
      "Confusion matrix(tn, fp, fn, tp): [190   2   1   7]\n",
      "Precision: 0.7777777777777778\n",
      "Recall: 0.875\n",
      "F1 score: 0.823529411764706\n",
      "lbfgs_two_layers\n",
      "Confusion matrix(tn, fp, fn, tp): [189   3   1   7]\n",
      "Precision: 0.7\n",
      "Recall: 0.875\n",
      "F1 score: 0.7777777777777777\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Problem 5\n",
    "X = transform(createPreviousumberOfRequestData(raw_data, 3))\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,  shuffle=False)\n",
    "X_test = transform(createPreviousumberOfRequestData(test, 3))\n",
    "TrainModel(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d931d54",
   "metadata": {},
   "source": [
    "###### Results\n",
    "    One layers lbfgs:\n",
    "    Confusion matrix(tn, fp, fn, tp): [190   2   1   7]\n",
    "    Precision: 0.7777777777777778\n",
    "    Recall: 0.875\n",
    "    F1 score: 0.823529411764706\n",
    "    Two layers lbfgs:\n",
    "    Confusion matrix(tn, fp, fn, tp): [189   3   1   7]\n",
    "    Precision: 0.7\n",
    "    Recall: 0.875\n",
    "    F1 score: 0.7777777777777777\n",
    "Creating new feature as normalized sum of previous request gives us respectful model, that mistakes only 1 false negative(worst scenario of not detecting a crash). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "421bef61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix(tn, fp, fn, tp): [185   7   6   2]\n",
      "Precision: 0.2222222222222222\n",
      "Recall: 0.25\n",
      "F1 score: 0.23529411764705882\n"
     ]
    }
   ],
   "source": [
    "#Fuzzy problem\n",
    "X_test = transform(createPreviousumberOfRequestData(test, 3))\n",
    "y_test = test.iloc[:, 2].to_numpy()\n",
    "\n",
    "crashing.input['load'] = X_test[:,0]\n",
    "crashing.input['sum_request'] = X_test[:,1]\n",
    "crashing.compute()\n",
    "pred=fuzz.lambda_cut(crashing.output['crash'], 0.5)\n",
    "print(\"Confusion matrix(tn, fp, fn, tp):\", confusion_matrix(y_test, pred).ravel())\n",
    "print(\"Precision:\",precision_score(y_test, pred ,zero_division=0))\n",
    "print(\"Recall:\",recall_score(y_test, pred))\n",
    "print(\"F1 score:\",f1_score(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d4d099",
   "metadata": {},
   "source": [
    "###### Results\n",
    "    Confusion matrix(tn, fp, fn, tp): [187   5   7   1]\n",
    "    Precision: 0.16666666666666666\n",
    "    Recall: 0.125\n",
    "    F1 score: 0.14285714285714288\n",
    "    \n",
    "Unfortunetly our fuzzy base system doesn't work with test data. Repeat of creation of fuzzy sets and rules is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f410cf",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Creating model for data with imbalanced data is challenging, but with knowledge of experts in domain we are able to get accurate predictions. Simple brute-force MLP rarely is enough. We have to use additional information as input in such imbalanced data or create new feature using previous timesteps. Recall is the most usefull score among accuracy and precision, but F1 score is also useful. Unfortunetly fuzzy this time failed, but it doesn't mean that it wouldn't suit this problem. Propably person creating Expert System isn't fuzzy person."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
